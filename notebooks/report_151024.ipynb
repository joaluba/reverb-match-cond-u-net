{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics based on the reverb part of audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT LIBRARIES ##################\n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display, HTML\n",
    "import torch\n",
    "from os.path import join as pjoin\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT MY MODULES ##################\n",
    "sys.path.append('../src')\n",
    "\n",
    "import helpers as hlp\n",
    "import evaluation\n",
    "import dataset as ds\n",
    "import trainer\n",
    "import models\n",
    "import loss_mel, loss_stft, loss_waveform, loss_embedd\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(hlp)\n",
    "importlib.reload(ds)\n",
    "importlib.reload(trainer)\n",
    "importlib.reload(models)\n",
    "importlib.reload(loss_mel)\n",
    "importlib.reload(loss_stft)\n",
    "importlib.reload(loss_waveform)\n",
    "importlib.reload(loss_embedd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Retrieving early and late part of the ground truth signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVING EARLY AND LATE PART OF THE GROUND TRUTH SIGNAL\n",
    "\n",
    "config=hlp.load_config(pjoin(\"/home/ubuntu/joanna/reverb-match-cond-u-net/config/basic.yaml\"))\n",
    "# instantiate a test data set \n",
    "config[\"split\"]=\"test\"\n",
    "dataset=ds.DatasetReverbTransfer(config)\n",
    "\n",
    "# Get \"extended\" data point i.e. all ground truth signals decomposed into early and late \n",
    "# and the corresponding room impulse responses\n",
    "hlp.init_random_seeds(0)\n",
    "INDX=46\n",
    "sigs, rirs = dataset.get_item_test(INDX,truncate_rirs=True)\n",
    "\n",
    "print(f\"{sigs.keys()=}\")\n",
    "print(f\"{rirs.keys()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Impulse responses \n",
    "- cut the intitial silence defined by 20dB threshold\n",
    "- scale so that the max peak = 1 \n",
    "- We divide full rir into early part and late part: \n",
    "- *rir<sub>full</sub> = rir<sub>early</sub> + rir<sub>late</sub>* \n",
    "- *rir<sub>early</sub>*  -> everything apart from the first peak (so direct sound)\n",
    "- *rir<sub>late</sub>*   ->  rest of the rir \n",
    "\n",
    "Below, we have 3 RIRs: \n",
    "- r1 (rir of the content sound)\n",
    "- r2 (rir of the style and target sound)\n",
    "- r2b (cloned rir, same room as r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rirs(early, late, together,suptitle):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    plt.subplot(1,3,1); plt.plot(early.T, color=\"red\"); plt.xlim([0,4000]) ; plt.ylim([-0.1,1]); plt.title(\"direct sound\")\n",
    "    plt.subplot(1,3,2); plt.plot(late.T, color=\"blue\") ; plt.xlim([0,4000]) ; plt.ylim([-0.1,1]) ;plt.title(\"all reflections\")\n",
    "    plt.subplot(1,3,3); plt.plot(together.T); plt.xlim([0,4000]) ; plt.ylim([-0.1,1]) ;plt.title(\"full rir\")\n",
    "    plt.suptitle(suptitle);plt.tight_layout();plt.show() \n",
    "\n",
    "plot_rirs(rirs[\"rirContent_early\"].numpy(),rirs[\"rirContent_late\"].numpy(),rirs[\"rirContent\"].numpy(), \"RIR of the content sound (r1)\")\n",
    "plot_rirs(rirs[\"rirTarget_early\"].numpy(),rirs[\"rirTarget_late\"].numpy(),rirs[\"rirTarget\"].numpy(), \"RIR of the target sound (r2)\")\n",
    "plot_rirs(rirs[\"rirTargetClone_early\"].numpy(),rirs[\"rirTargetClone_late\"].numpy(),rirs[\"rirTargetClone\"].numpy(), \"RIR of the cloned target sound (r2b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Audio signals\n",
    "\n",
    "- We divide full reverberant signal into early part and late part\n",
    "- full signal: *s<sub>full</sub>  = s<sub>early</sub>  + s<sub>late</sub>* \n",
    "- We scale the output so that max peak = 1 (scaling factor sc) -> the network expects a waveform in the range [-1,1]\n",
    "- *sc * s<sub>full</sub>   = sc * s<sub>early</sub> + sc * s<sub>late</sub>* \n",
    "\n",
    "\n",
    "Below we have the following signals:\n",
    "- s1r1 (sContent)\n",
    "- s1r2 (sTarget)\n",
    "- s1r2b (sTargetClone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sigs(early, late, together, suptitle):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.subplot(1,3,1); plt.plot(early.T, color=\"red\"); plt.ylim(-1,1); plt.title(\"direct sound\")\n",
    "    plt.subplot(1,3,2); plt.plot(late.T, color=\"blue\"); plt.ylim(-1,1); plt.title(\"all reflections\")\n",
    "    plt.subplot(1,3,3); plt.plot(together.T); plt.ylim(-1,1); plt.title(\"full signal\")\n",
    "    plt.suptitle(suptitle);plt.tight_layout();plt.show() \n",
    "\n",
    "def audio_3_sigs(early,late, together):\n",
    "    au1=Audio(early,rate=48000)\n",
    "    au2=Audio(late,rate=48000)\n",
    "    au3=Audio(together,rate=48000)\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"display: flex; space-between;\">\n",
    "        <div>{au1._repr_html_()}</div>\n",
    "        <div>{au2._repr_html_()}</div>\n",
    "        <div>{au3._repr_html_()}</div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "plot_sigs(sigs[\"sContent_early\"].numpy(),sigs[\"sContent_late\"].numpy(),sigs[\"sContent\"].numpy(), \"Content sound (s1r1)\")\n",
    "audio_3_sigs(sigs[\"sContent_early\"].numpy(),sigs[\"sContent_late\"].numpy(),sigs[\"sContent\"].numpy())\n",
    "\n",
    "plot_sigs(sigs[\"sTarget_early\"].numpy(),sigs[\"sTarget_late\"].numpy(),sigs[\"sTarget\"].numpy(), \"Target sound (s1r2)\")\n",
    "audio_3_sigs(sigs[\"sTarget_early\"].numpy(),sigs[\"sTarget_late\"].numpy(),sigs[\"sTarget\"].numpy())\n",
    "\n",
    "plot_sigs(sigs[\"sTargetClone_early\"].numpy(),sigs[\"sTargetClone_late\"].numpy(),sigs[\"sTargetClone\"].numpy(), \"Target clone sound (s1r2b)\")\n",
    "audio_3_sigs(sigs[\"sTargetClone_early\"].numpy(),sigs[\"sTargetClone_late\"].numpy(),sigs[\"sTargetClone\"].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Subtracting early part to estimate the late part (non-processed signals)\n",
    "\n",
    "- Knowing early part of the target signal (*s1r2<sub>early</sub>*), we can subtract it from a full reverberant signal to get the estimate of the late part.\n",
    "- For example, if we have an estimate of target, we can calculate:\n",
    "\n",
    "$$\\widehat{s1r2}_{late}= \\widehat{s1r2} - s1r2_{early}$$\n",
    "\n",
    "- ...and we get the estimate of the late part of the target.\n",
    "\n",
    "\n",
    "- Below we do this for sTargetClone and sContent (we estimate the late part): \n",
    "\n",
    "$$\\widehat{s1r1}_{late}= s1r1 - s1r2_{early}$$\n",
    "$$\\widehat{s1r2b}_{late}= s1r2b - s1r2_{early}$$\n",
    "\n",
    "- ...and we compare the estimate of the late part to the actual late part (we know the actual part because we have ground truth knowledge for these signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gt_est(gt, est, suptitle, colorchoice):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.subplot(1,2,1); plt.plot(gt.T, color=colorchoice); plt.ylim(-1,1); plt.title(\"ground truth\")\n",
    "    plt.subplot(1,2,2); plt.plot(est.T, color=colorchoice); plt.ylim(-1,1); plt.title(\"estimate\")\n",
    "    plt.suptitle(suptitle);plt.tight_layout();plt.show() \n",
    "\n",
    "def audio_2_sigs(s1,s2):\n",
    "    au1=Audio(s1,rate=48000)\n",
    "    au2=Audio(s2,rate=48000)\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"display: flex; space-between;\">\n",
    "        <div>{au1._repr_html_()}</div>\n",
    "        <div>{au2._repr_html_()}</div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "# estimate of the late part of sTargetClone\n",
    "sTargetClone_late_gt=sigs[\"sTargetClone_late\"].numpy()\n",
    "sTargetClone_late_estim=sigs[\"sTargetClone\"].numpy()- sigs[\"sTarget_early\"].numpy()\n",
    "plot_gt_est(sTargetClone_late_gt, sTargetClone_late_estim, \"Estimating late part of sTargetClone\",\"blue\")\n",
    "audio_2_sigs(sTargetClone_late_gt,sTargetClone_late_estim)\n",
    "\n",
    "\n",
    "# estimate of the late part of sContent\n",
    "sContent_late_gt=sigs[\"sContent_late\"].numpy()\n",
    "sContent_late_estim=sigs[\"sContent\"].numpy()- sigs[\"sTarget_early\"].numpy()\n",
    "plot_gt_est(sContent_late_gt, sContent_late_estim, \"Estimating late part of sContent\",\"blue\")\n",
    "audio_2_sigs(sContent_late_gt,sContent_late_estim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This works quite well. The estimate of the late part is perceptually very close to the ground truth of the late part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Subtracting early part to estimate the late part (processed signals)\n",
    "\n",
    "- so far we have only looked at signals that are not processed by any network\n",
    "- now we will generate the estimated of the target using different models\n",
    "- next, we will estimate the late part according to the formula: \n",
    "\n",
    "$\\widehat{s1r2}_{late}= \\widehat{s1r2} - s1r2_{early}$\n",
    "\n",
    "- and we will compare by listening:  \n",
    "- ${s1r2}$ (target ground truth)   \n",
    "- $\\widehat{s1r2}$ (target estimate)  \n",
    "- $s1r2_{late}$ (target late part ground truth)  \n",
    "- $\\widehat{s1r2}_{late}$  (target late part estimate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT EVALUATION OBJECT (CONTAINS FUNCTION THAT GETS PREDICTIONS FROM DIFFERENT MODELS & TO COMPUTE METRICS)\n",
    "\n",
    "myeval = evaluation.Evaluator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE AND SAVE TO WAV ALL VERSIONS OF A DATAPOINT (GROUND TRUTH, BASELINES, OUR MODEL PREDICTIONS)\n",
    "\n",
    "# which data point to use\n",
    "wavsavedir=\"../sounds/report_151024/\"\n",
    "resave=True\n",
    "\n",
    "# make a list of checkpoints to compare (in addition to ground truth sounds and baselines)\n",
    "checkpoint_paths=[\n",
    "                  \"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/10-06-2024--15-02_c_wunet_stft+wave_0.8_0.2/checkpointbest.pt\",\n",
    "                  \"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/20-05-2024--22-48_c_wunet_logmel+wave_0.8_0.2/checkpointbest.pt\",\n",
    "                  \"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/29-05-2024--05-47_c_wunet_logmel_1/checkpointbest.pt\",\n",
    "                  \"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/18-06-2024--18-37_c_wunet_stft_1/checkpointbest.pt\",\n",
    "                  \"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/18-06-2024--18-37_c_wunet_stft_1/checkpoint50.pt\",\n",
    "                  \"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/18-06-2024--18-37_c_wunet_stft_1/checkpoint10.pt\",\n",
    "                  \"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/18-06-2024--18-37_c_wunet_stft_1/checkpoint0.pt\"\n",
    "                  ]\n",
    "\n",
    "# init dictionaries to contain signals and corresponding file names\n",
    "sigs={}\n",
    "filenames={}\n",
    "\n",
    "# get ground truth signals for this sample\n",
    "sigs_gt,filenames_gt=myeval.save_audios_sample_ext(\"groundtruth\",INDX,wavsavedir,savefiles=resave)\n",
    "sigs.update(sigs_gt)\n",
    "filenames.update(filenames_gt)\n",
    "\n",
    "# get baseline predictions for this sample\n",
    "sigs_bl,filenames_bl=myeval.save_audios_sample_ext(\"baselines\",INDX,wavsavedir,savefiles=resave)\n",
    "sigs.update(sigs_bl)\n",
    "filenames.update(filenames_bl)\n",
    "\n",
    "# get checkpoint predictions for this sample\n",
    "for i, checkpoint_path in enumerate(checkpoint_paths):\n",
    "    sigs_chckpt,filenames_chckpt=myeval.save_audios_sample_ext(checkpoint_path,INDX,wavsavedir,savefiles=resave)\n",
    "    sigs.update(sigs_chckpt)\n",
    "    filenames.update(filenames_chckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY THE OUTPUT OF THE ABOVE FUNCTION (SIGNALS AND CORRESPONDING FILE PATHS)\n",
    "\n",
    "for key in list(sigs.keys()):\n",
    "    print(key + \" --->  \" + filenames[key]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT AND PLAY BACK TARGET AND ESTIMATES\n",
    "\n",
    "def audio_4_sigs(s1,s2,s3,s4):\n",
    "    au1=Audio(s1,rate=48000)\n",
    "    au2=Audio(s2,rate=48000)\n",
    "    au3=Audio(s3,rate=48000)\n",
    "    au4=Audio(s4,rate=48000)\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"display: flex; space-between;\">\n",
    "        <div>{au1._repr_html_()}</div>\n",
    "        <div>{au2._repr_html_()}</div>\n",
    "        <div>{au3._repr_html_()}</div>\n",
    "        <div>{au4._repr_html_()}</div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "audio1=sigs[\"sTarget\"].numpy()\n",
    "audio2=sigs[\"sTarget_late\"].numpy()\n",
    "\n",
    "for key in list(sigs.keys()):\n",
    "    if (\"_late\" in key) and (\"sPred\" in key):\n",
    "        key_full=key.split(\"_late\")[0]\n",
    "        audio3=sigs[key_full].numpy()\n",
    "        audio4=sigs[key].numpy()\n",
    "        plot_gt_est(audio1, audio3, \"Comparing:  sTarget to \" + key_full + \" (FULL SIG)\", \"steelblue\")\n",
    "        # hlp.plot_2_spectrograms(audio1, audio3,  48000)\n",
    "        audio_2_sigs(audio1,audio3)\n",
    "        plot_gt_est(audio2, audio4, \"Comparing:  sTarget to \"  + key_full + \" (LATE PART)\", \"blue\")\n",
    "        # hlp.plot_2_spectrograms(audio2, audio4,  48000)\n",
    "        audio_2_sigs(audio2,audio4)\n",
    "        print(\"----------------------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For natural signal this way of estimating the late reverb worked quite ok\n",
    "- For some processed signals (especially logmel-trained) we get something a bit similar to the late part\n",
    "- But for stft-trained signals, subtracting early part gives a signal with a very different dynamic range than ground truth late reverb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Using metrics on the late part\n",
    "\n",
    "- same metrics as the one that I applied to the full signal:\n",
    "\n",
    "- Type 1: similarity measured using symmetric metric : **M = m(a,b) = m(b,a)**\n",
    "    - '1L_multi-stft-mag'\n",
    "    - '1L_stft'\n",
    "    - '1L_stft-mag'\n",
    "    - '1L_multi-wave'\n",
    "    - '1L_wave'\n",
    "    - '1L_logmel'\n",
    "    - '1L_multi-mel'\n",
    "    - '1S_sisdr'\n",
    "    - '1L_emb_euc'\n",
    "- Type 2: similarity measured using non-symmetric metric: **M = (m(a,b)+m(b,a))/2**\n",
    "    - '2L_lsd' \n",
    "    - '2L_mcd' \n",
    "    - '2S_fwsnr'\n",
    "    - '2L_multi-stft'\n",
    "    - '2L_stft'\n",
    "    - '2S_pesq'\n",
    "    - '2S_stoi'\n",
    "- Type 3: similarity measured as distance in intrusive metric: **M = abs(m(a,ref) - m(b,ref))**\n",
    "    - '3D_pesq'\n",
    "    - '3D_stoi'\n",
    "    - '3D_sisdr'\n",
    "    - '3D_mos_nidiff'\n",
    "    - '3D_pesq_nidiff'\n",
    "    - '3D_stoi_nidiff'\n",
    "    - '3D_sisdr_nidiff'\n",
    "\n",
    "- for late parts, I scaled all the signals before they enter the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## LOAD CSV WITH EVALUATION RESULTS ##################\n",
    "df=pd.read_csv(\"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/151024_compare_percept_100testset_revpart_scaled.csv\")\n",
    "display(df.head(5))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## GIVE MORE CONCISE TAGS FOR EACH CATEGORY ##################\n",
    "\n",
    "def impove_categories_tags(df): \n",
    "       # add a column to store a shorter tag identifying each category\n",
    "       df['short_label'] = df['label'].apply(lambda x: x.split('_', 1)[1] if \"_\" in x else x)\n",
    "       df['short_label'] = df['short_label'].apply(lambda x: x.replace(\"checkpoint\",\"ch\"))\n",
    "       df['tag'] = df['short_label']+ ' -> ' + df['compared']\n",
    "       df['tag'] = df['tag'].apply(lambda x: x.replace(\"target\",\"tar\"))\n",
    "       df['tag'] = df['tag'].apply(lambda x: x.replace(\"prediction\",\"pred\"))\n",
    "       df=df.sort_values(\"compared\")\n",
    "       df=df.drop(columns=['short_label'])\n",
    "       # create a custom order of the files so that the plots have similar order as before\n",
    "       custom_order=[\"oracle -> tar:tarclone\" , \"oracle -> r(tar):r(tarclone)\", \n",
    "                     \"oracle -> tar:content\", \"oracle -> r(tar):r(content)\",  \n",
    "                     \"anecho+fins -> pred:tar\", \"anecho+fins -> r(pred):r(tar)\", \n",
    "                     \"dfnet+fins -> pred:tar\",  \"dfnet+fins -> r(pred):r(tar)\",\n",
    "                     \"wpe+fins -> pred:tar\", \"wpe+fins -> r(pred):r(tar)\",\n",
    "                     \"c_wunet_stft+wave_0.8_0.2_chbest -> pred:tar\", \"c_wunet_stft+wave_0.8_0.2_chbest -> r(pred):r(tar)\", \n",
    "                     \"c_wunet_logmel+wave_0.8_0.2_chbest -> pred:tar\", \"c_wunet_logmel+wave_0.8_0.2_chbest -> r(pred):r(tar)\", \n",
    "                     \"c_wunet_logmel_1_chbest -> pred:tar\", \"c_wunet_logmel_1_chbest -> r(pred):r(tar)\", \n",
    "                     \"c_wunet_stft_1_chbest -> pred:tar\",\"c_wunet_stft_1_chbest -> r(pred):r(tar)\",\n",
    "                     \"c_wunet_stft_1_ch50 -> pred:tar\", \"c_wunet_stft_1_ch50 -> r(pred):r(tar)\", \n",
    "                     \"c_wunet_stft_1_ch10 -> pred:tar\", \"c_wunet_stft_1_ch10 -> r(pred):r(tar)\", \n",
    "                     \"c_wunet_stft_1_ch0 -> pred:tar\", \"c_wunet_stft_1_ch0 -> r(pred):r(tar)\"]\n",
    "       df['tag'] = pd.Categorical(df['tag'], categories=custom_order, ordered=True)\n",
    "       df=df.sort_values(\"tag\")\n",
    "       return df\n",
    "\n",
    "df=impove_categories_tags(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## ADD COLUMN TO KNOW IF THE SAMPLE IS REV2DRY OR DRY2REV ##################\n",
    "\n",
    "# divide into re-reverbaration, de-reverberation \n",
    "config=hlp.load_config(pjoin(\"/home/ubuntu/joanna/reverb-match-cond-u-net/config/basic.yaml\"))\n",
    "\n",
    "def get_reverb_ind(config, df, split):\n",
    "    config[\"p_noise\"]=0\n",
    "    config[\"split\"]=split\n",
    "    dataset=ds.DatasetReverbTransfer(config)\n",
    "    indices_dry2rev=dataset.get_idx_with_rt60diff(-3,-0.3)\n",
    "    indices_rev2dry=dataset.get_idx_with_rt60diff(0.3,3)\n",
    "    indices_smalldiff=dataset.get_idx_with_rt60diff(-0.3,0.3)\n",
    "    df.loc[df[\"idx\"].isin(indices_dry2rev), \"rev_delta\"] = \"dry2rev\"\n",
    "    df.loc[df[\"idx\"].isin(indices_rev2dry), \"rev_delta\"] = \"rev2dry\"\n",
    "    df.loc[df[\"idx\"].isin(indices_smalldiff), \"rev_delta\"] = \"smalldiff\"\n",
    "    return df \n",
    "\n",
    "df=get_reverb_ind(config, df, \"test\")\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter results into full signal and late reverb\n",
    "df_late=df[df[\"tag\"].str.contains(\"r\\(\")]\n",
    "df_full=df[~df[\"tag\"].str.contains(\"r\\(\")]\n",
    "print(len(df_late))\n",
    "print(len(df_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### FOR EACH SAMPLE INDEX, CHECK IF FOR THAT SAMPLE THE PERCEPTUAL EXPECTATIONS ARE CONFIRMED ##################\n",
    "\n",
    "def check_requirements(df,idx):\n",
    "\n",
    "    # get the names and number of metrics used\n",
    "    df_metrics_only = df.loc[:, ~df.columns.isin([\"label\",\"idx\",\"compared\",\"short_label\",\"tag\",'rev_delta','dataset'])]\n",
    "    N_metrics=len(df_metrics_only.columns)\n",
    "\n",
    "    # get data for one index (for each index several audios: anecho, content, target, processed, etc.)\n",
    "    df_idx=df[df[\"idx\"]==idx]\n",
    "\n",
    "    rev_delta=df_idx['rev_delta'].iloc[0]\n",
    "\n",
    "    # create dictionary to fill for this index\n",
    "    conditions_dict={\"metric\": list(df_metrics_only.columns),\n",
    "                     \"idx\" : [idx] * N_metrics,\n",
    "                     \"rev_delta\" : [rev_delta] * N_metrics,\n",
    "                     \"sTargetClone > sContent\" : [False] * N_metrics,\n",
    "                     \"sTargetClone most similar\" : [False] * N_metrics,\n",
    "                     \"sContent < sPred_anecho_fins & our best models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > all our models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > our logmel models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > our stft models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > our half-trained models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\" :[False] * N_metrics,\n",
    "                     \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\" : [False] * N_metrics,\n",
    "                     \"earlier checkpoints < later checkpoints\" : [False] * N_metrics}\n",
    "    \n",
    "    def note_metric_in_conddict(conditions_dict, key):\n",
    "        for j,metric in enumerate(conditions_dict[\"metric\"]):\n",
    "            if metric==col: \n",
    "                conditions_dict[key][j]=True\n",
    "\n",
    "    # filtering\n",
    "    df_idx=df_idx[df_idx[\"tag\"]!= \"tar:anecho\"]\n",
    "    df_idx=df_idx[df_idx[\"tag\"]!= \"tar:style\"]\n",
    "\n",
    "    row_content = df_idx[df_idx['tag'].str.contains(\"content\")]\n",
    "    row_targetclone = df_idx[df_idx['tag'].str.contains(\"tarclone\")]\n",
    "\n",
    "    row_anechofins = df_idx[df_idx['tag'].str.contains(\"anecho\\\\+fins\")]\n",
    "    row_dfnetfins = df_idx[df_idx['tag'].str.contains(\"dfnet\\\\+fins\")]\n",
    "    row_wpefins = df_idx[df_idx['tag'].str.contains(\"wpe\\\\+fins\")]\n",
    "\n",
    "    rows_fins_notanecho=pd.concat([row_dfnetfins,row_wpefins])\n",
    "\n",
    "    row_ourlogmelwave = df_idx[df_idx['tag'].str.contains(\"logmel\\\\+wave_0.8_0.2_chbest\")]\n",
    "    row_ourstftwave = df_idx[df_idx['tag'].str.contains(\"stft\\\\+wave_0.8_0.2_chbest\")]\n",
    "    row_ourlogmel = df_idx[df_idx['tag'].str.contains(\"logmel_1_chbest\")]\n",
    "    row_ourstft = df_idx[df_idx['tag'].str.contains(\"stft_1_chbest\")]\n",
    "\n",
    "    rows_ourbests=pd.concat([row_ourlogmelwave,row_ourstftwave, row_ourlogmel,row_ourstft])\n",
    "\n",
    "    row_ourstft0 = df_idx[df_idx['tag'].str.contains(\"ch0\")]\n",
    "    row_ourstft10 = df_idx[df_idx['tag'].str.contains(\"ch10\")]\n",
    "    row_ourstft50 = df_idx[df_idx['tag'].str.contains(\"ch50\")]\n",
    "\n",
    "    rows_ourhalftrained=pd.concat([row_ourstft0,row_ourstft10])\n",
    "    rows_ourmodels = df_idx[df_idx['tag'].str.contains(\"_ch\")]\n",
    "    rows_ourlogmels=df_idx[df_idx['tag'].str.contains(\"logmel\") & df_idx['tag'].str.contains(\"best\")]\n",
    "    rows_ourstfts=df_idx[df_idx['tag'].str.contains(\"stft\") & df_idx['tag'].str.contains(\"best\")]\n",
    "    rows_bestpreds = pd.concat([rows_ourbests,row_anechofins])\n",
    "\n",
    "    \n",
    "    # sTargetClone is more similar to sTarget than sContent (change in reverb)\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (row_content[col].item() >= row_targetclone[col].item()):  \n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone > sContent\")\n",
    "        elif \"S_\" in col:\n",
    "            if (row_content[col].item() <= row_targetclone[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone > sContent\")\n",
    "\n",
    "    # sTargetClone is most similar to sTarget from all the signals\n",
    "    for col in df_idx.columns:\n",
    "        sorted_values = df_idx.sort_values(by=col).reset_index(drop=True)\n",
    "        # loss/difference metrics\n",
    "        if \"L_\" in col  or \"D_\" in col: \n",
    "            if \"tarclone\" in sorted_values.loc[sorted_values.index[0], 'tag']:  # Check second lowest\n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone most similar\")\n",
    "        # similarity metrics\n",
    "        elif \"S_\" in col: \n",
    "            if \"tarclone\" in sorted_values.loc[sorted_values.index[-1], 'tag']:  # Check second lowest\n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone most similar\")\n",
    "\n",
    "\n",
    "    # sContent less similar to target than sPred_anecho_fins and our well-trained models\n",
    "    # i.e. transformation helps\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_bestpreds[col] < row_content[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sContent < sPred_anecho_fins & our best models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_bestpreds[col] > row_content[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sContent < sPred_anecho_fins & our best models\")\n",
    "\n",
    "\n",
    "    # sPred_anecho_fins is better than all of our models\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_ourmodels[col] >= row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > all our models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_ourmodels[col] <= row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > all our models\")\n",
    "\n",
    "    # sPred_anecho_fins is at least better than logmel model \n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_ourlogmels[col] >= row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our logmel models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_ourlogmels[col] <= row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our logmel models\")\n",
    "\n",
    "    # sPred_anecho_fins is at least better than stft models\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_ourstfts[col] >= row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our stft models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_ourstfts[col] <= row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our stft models\")\n",
    "\n",
    "    # sPred_anecho_fins is at least better than our half-trained models \n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_ourhalftrained[col] >= row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our half-trained models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_ourhalftrained[col] <= row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our half-trained models\")\n",
    "\n",
    "    # sPred_anecho_fins is better than sPred_dfnet_fins and sPred_wpe_fins\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_fins_notanecho[col] > row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_fins_notanecho[col] < row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\")\n",
    "\n",
    "\n",
    "    # stft+wave_0.8_0.2_checkpointbest better than logmel+wave_0.8_0.2_checkpointbest\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (row_ourlogmel[col].item() > row_ourstft[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\")\n",
    "        elif \"S_\" in col:\n",
    "            if (row_ourlogmel[col].item() < row_ourstft[col].item()):  \n",
    "                note_metric_in_conddict(conditions_dict, \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\")\n",
    "\n",
    "    # checkpoint0 <  checkpoint10 < checkpoint50\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (row_ourstft0[col].item() > row_ourstft10[col].item() and \n",
    "            row_ourstft10[col].item() > row_ourstft50[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"earlier checkpoints < later checkpoints\")\n",
    "        elif \"S_\" in col:\n",
    "            if (row_ourstft0[col].item() < row_ourstft10[col].item() and \n",
    "            row_ourstft10[col].item() < row_ourstft50[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"earlier checkpoints < later checkpoints\")\n",
    "\n",
    "    return conditions_dict\n",
    "    # end of function \"check_requirements(df,idx)\"\n",
    "\n",
    "def compute_metrics_table(df):\n",
    "    # get unique indices\n",
    "    unique_idx=df.idx.unique()\n",
    "    # initialize empty dictionary\n",
    "    keys=[\"metric\",\n",
    "        \"idx\",\n",
    "        \"rev_delta\",\n",
    "        \"sTargetClone > sContent\",\n",
    "        \"sTargetClone most similar\",\n",
    "        \"sContent < sPred_anecho_fins & our best models\" ,\n",
    "        \"sPred_anecho_fins > all our models\" ,\n",
    "        \"sPred_anecho_fins > our logmel models\" ,\n",
    "        \"sPred_anecho_fins > our stft models\" ,\n",
    "        \"sPred_anecho_fins > our half-trained models\"  ,\n",
    "        \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\" ,\n",
    "        \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\" ,\n",
    "        \"earlier checkpoints < later checkpoints\" ]\n",
    "    combined_dict =  {key: [] for key in keys}\n",
    "    i=0\n",
    "    for idx in unique_idx:\n",
    "        i+=1\n",
    "        tmp_dict=check_requirements(df,idx)\n",
    "        combined_dict = {key: combined_dict[key] + tmp_dict[key] for key in keys}\n",
    "    # go from dictionary to df\n",
    "    df_table=pd.DataFrame(combined_dict)\n",
    "    return df_table\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_table_full=compute_metrics_table(df_full)\n",
    "df_table_late=compute_metrics_table(df_late)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  CREATE A TABLE INDICATING WHICH PERCEPTUAL OBSERVATIONS ARE CONFIRMED BY EACH METRIC ################### \n",
    "# CASE 1 -> FULL SIGNAL (SAME AS BEFORE)\n",
    "\n",
    "needed_columns = df_table_full.drop(columns=[\"idx\",\"rev_delta\"],axis=1)\n",
    "df_table_g=needed_columns.groupby(\"metric\").agg(lambda x: int((x.sum() / len(x))*100)).reset_index()\n",
    "df_table_g = df_table_g.style.background_gradient(cmap='viridis', vmin=0, vmax=100,)\n",
    "display(df_table_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  CREATE A TABLE INDICATING WHICH PERCEPTUAL OBSERVATIONS ARE CONFIRMED BY EACH METRIC ################### \n",
    "# CASE 1 -> LATE\n",
    "\n",
    "needed_columns = df_table_late.drop(columns=[\"idx\",\"rev_delta\"],axis=1)\n",
    "df_table_g=needed_columns.groupby(\"metric\").agg(lambda x: int((x.sum() / len(x))*100)).reset_index()\n",
    "df_table_g = df_table_g.style.background_gradient(cmap='viridis', vmin=0, vmax=100,)\n",
    "display(df_table_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in general, the metrics \"see\" that the late reverb of targetclone is closer to the target than the late reverb of the content sound\n",
    "- but (surprisingly) this difference is less clear than when using late part vs when using full signals \n",
    "- also less metrics show that reverb transfer helps (that the signal after transformation becomes more similar to the target)\n",
    "- even though we use only the reverb part for analysis, the baseline is still not better than our networks\n",
    "- but at least some metrics consider baseline better than our half-trained models\n",
    "- if we cut the direct sound from the signals, there is no difference between early and late checkpoints (which could mean that during training the direct sound is improved but the reverb not much)\n",
    "- so it doesnt look like it gives us a better information to evaluate the signals on the late part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT AVERAGE METRICS \n",
    "\n",
    "# create df with average metric across data points\n",
    "numeric_df_late = df_late.select_dtypes(include='number')\n",
    "numeric_df_full = df_full.select_dtypes(include='number')\n",
    "\n",
    "df_g_late=df_late.groupby(\"tag\")[numeric_df_late.columns].mean().reset_index()\n",
    "df_g_full=df_full.groupby(\"tag\")[numeric_df_full.columns].mean().reset_index()\n",
    "\n",
    "# for some reason df_late.tag.unique() gives different list then df_late.groupby([\"tag\"]).groups.keys()\n",
    "# so the workaround is to remove the empty rows of df_g \n",
    "df_g_late=df_g_late.dropna()\n",
    "df_g_full=df_g_full.dropna()\n",
    "\n",
    "# plot average metrics\n",
    "plt.figure(figsize=(10,150))\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "N_metrics=len(df_g_late.columns)\n",
    "i=0\n",
    "for column in df_g_late.columns:\n",
    "    if column not in [\"label\", \"idx\", \"compared\",\"short_label\",\"tag\"]: \n",
    "        N_rows=int(np.ceil(N_metrics))\n",
    "        metriccolor=\"red\" if \"D_\" in column or \"L_\" in column else \"green\"\n",
    "        # plot metrics on full signal\n",
    "        plt.subplot(N_rows,2,i+1)\n",
    "        bars=plt.bar(df_g_full[\"tag\"],df_g_full[column], color=metriccolor)\n",
    "        plt.xticks(rotation=60, ha='right', fontsize=\"8\")\n",
    "        plt.title(\"Metric mean: \" + column + \" (FULL SIG)\")\n",
    "        for j,bar in enumerate(bars):\n",
    "            val4clone=bars[j].get_height()#df_idx[column][df_idx[\"label\"]==\"sTarget : sAnecho\"][0]\n",
    "            plt.text(j,val4clone,str(\"%.2f\" % val4clone), horizontalalignment='center',verticalalignment='bottom', fontsize=\"7\")\n",
    "        # plot metrics on late part of the signal\n",
    "        plt.subplot(N_rows,2,i+2)\n",
    "        bars=plt.bar(df_g_late[\"tag\"],df_g_late[column], color=metriccolor)\n",
    "        plt.xticks(rotation=60, ha='right', fontsize=\"8\")\n",
    "        plt.title(\"Metric mean: \" + column + \" (LATE PART)\")\n",
    "        for j,bar in enumerate(bars):\n",
    "            val4clone=bars[j].get_height()#df_idx[column][df_idx[\"label\"]==\"sTarget : sAnecho\"][0]\n",
    "            plt.text(j,val4clone,str(\"%.2f\" % val4clone), horizontalalignment='center',verticalalignment='bottom', fontsize=\"7\")\n",
    "\n",
    "        i=i+2\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wave-u-net2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
