{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which metrics correlate with perception? (multiple audio samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT LIBRARIES ##################\n",
    "import sys\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display, HTML\n",
    "import torch\n",
    "from os.path import join as pjoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT MY MODULES ##################\n",
    "sys.path.append('../src')\n",
    "\n",
    "import helpers as hlp\n",
    "import evaluation\n",
    "import dataset as ds\n",
    "import trainer\n",
    "import models\n",
    "import loss_mel, loss_stft, loss_waveform, loss_embedd\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(hlp)\n",
    "importlib.reload(ds)\n",
    "importlib.reload(trainer)\n",
    "importlib.reload(models)\n",
    "importlib.reload(loss_mel)\n",
    "importlib.reload(loss_stft)\n",
    "importlib.reload(loss_waveform)\n",
    "importlib.reload(loss_embedd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## LOAD CSV WITH EVALUATION RESULTS ##################\n",
    "df=pd.read_csv(\"/home/ubuntu/Data/RESULTS-reverb-match-cond-u-net/runs-exp-20-05-2024/100924_compare_percept_5000testset.csv\")\n",
    "display(df.head(-20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## GIVE MORE CONCISE TAGS FOR EACH CATEGORY ##################\n",
    "\n",
    "def impove_categories_tags(df): \n",
    "       # add a column to store a shorter tag identifying each category\n",
    "       df['short_label'] = df['label'].apply(lambda x: x.split('_', 1)[1] if \"_\" in x else x)\n",
    "       df['short_label'] = df['short_label'].apply(lambda x: x.replace(\"checkpoint\",\"ch\"))\n",
    "       df['tag'] = df['short_label']+ ' -> ' + df['compared']\n",
    "       df['tag'] = df['tag'].apply(lambda x: x.replace(\"target\",\"tar\"))\n",
    "       df['tag'] = df['tag'].apply(lambda x: x.replace(\"prediction\",\"pred\"))\n",
    "       df=df.sort_values(\"compared\")\n",
    "       df=df.drop(columns=['short_label'])\n",
    "       # create a custom order of the files so that the plots have similar order as before\n",
    "       custom_order=[\"oracle -> tar:anecho\", \"oracle -> tar:tarclone\" , \"oracle -> tar:content\", \"oracle -> tar:style\", \"anecho+fins -> pred:tar\", \"dfnet+fins -> pred:tar\", \"wpe+fins -> pred:tar\",\n",
    "              \"c_wunet_stft+wave_0.8_0.2_chbest -> pred:tar\", \"c_wunet_logmel+wave_0.8_0.2_chbest -> pred:tar\", \"c_wunet_logmel_1_chbest -> pred:tar\", \"c_wunet_stft_1_chbest -> pred:tar\",\n",
    "              \"c_wunet_stft_1_ch50 -> pred:tar\", \"c_wunet_stft_1_ch10 -> pred:tar\", \"c_wunet_stft_1_ch0 -> pred:tar\"]\n",
    "       df['tag'] = pd.Categorical(df['tag'], categories=custom_order, ordered=True)\n",
    "       df=df.sort_values(\"tag\")\n",
    "       return df\n",
    "\n",
    "df=impove_categories_tags(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## ADD COLUMN TO KNOW IF THE SAMPLE IS REV2DRY OR DRY2REV ##################\n",
    "\n",
    "# divide into re-reverbaration, de-reverberation \n",
    "config=hlp.load_config(pjoin(\"/home/ubuntu/joanna/reverb-match-cond-u-net/config/basic.yaml\"))\n",
    "\n",
    "def get_reverb_ind(config, df, split):\n",
    "    config[\"p_noise\"]=0\n",
    "    config[\"split\"]=split\n",
    "    dataset=ds.DatasetReverbTransfer(config)\n",
    "    indices_dry2rev=dataset.get_idx_with_rt60diff(-3,-0.3)\n",
    "    indices_rev2dry=dataset.get_idx_with_rt60diff(0.3,3)\n",
    "    indices_smalldiff=dataset.get_idx_with_rt60diff(-0.3,0.3)\n",
    "    df.loc[df[\"idx\"].isin(indices_dry2rev), \"rev_delta\"] = \"dry2rev\"\n",
    "    df.loc[df[\"idx\"].isin(indices_rev2dry), \"rev_delta\"] = \"rev2dry\"\n",
    "    df.loc[df[\"idx\"].isin(indices_smalldiff), \"rev_delta\"] = \"smalldiff\"\n",
    "    return df \n",
    "\n",
    "df=get_reverb_ind(config, df, \"test\")\n",
    "\n",
    "\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### FOR EACH SAMPLE INDEX, CHECK IF FOR THAT SAMPLE THE PERCEPTUAL EXPECTATIONS ARE CONFIRMED ##################\n",
    "\n",
    "def check_requirements(df,idx):\n",
    "\n",
    "    # get the names and number of metrics used\n",
    "    df_metrics_only = df.loc[:, ~df.columns.isin([\"label\",\"idx\",\"compared\",\"short_label\",\"tag\",'rev_delta','dataset'])]\n",
    "    N_metrics=len(df_metrics_only.columns)\n",
    "\n",
    "    # get data for one index (for each index several audios: anecho, content, target, processed, etc.)\n",
    "    df_idx=df[df[\"idx\"]==idx]\n",
    "\n",
    "    rev_delta=df_idx['rev_delta'].iloc[0]\n",
    "\n",
    "    # create dictionary to fill for this index\n",
    "    conditions_dict={\"metric\": list(df_metrics_only.columns),\n",
    "                     \"idx\" : [idx] * N_metrics,\n",
    "                     \"rev_delta\" : [rev_delta] * N_metrics,\n",
    "                     \"sTargetClone > sContent\" : [False] * N_metrics,\n",
    "                     \"sTargetClone most similar\" : [False] * N_metrics,\n",
    "                     \"sContent < sPred_anecho_fins & our best models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > all our models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > our logmel model\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > our half-trained models\" : [False] * N_metrics,\n",
    "                     \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\" :[False] * N_metrics,\n",
    "                     \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\" : [False] * N_metrics,\n",
    "                     \"earlier checkpoints < later checkpoints\" : [False] * N_metrics}\n",
    "    \n",
    "    def note_metric_in_conddict(conditions_dict, key):\n",
    "        for j,metric in enumerate(conditions_dict[\"metric\"]):\n",
    "            if metric==col: \n",
    "                conditions_dict[key][j]=True\n",
    "            \n",
    "\n",
    "\n",
    "    # filtering\n",
    "    df_idx=df_idx[df_idx[\"tag\"]!= \"tar:anecho\"]\n",
    "    df_idx=df_idx[df_idx[\"tag\"]!= \"tar:style\"]\n",
    "\n",
    "    row_content = df_idx[df_idx['tag'].str.contains(\"content\")]\n",
    "    row_targetclone = df_idx[df_idx['tag'].str.contains(\"tarclone\")]\n",
    "\n",
    "    row_anechofins = df_idx[df_idx['tag'].str.contains(\"anecho\\\\+fins\")]\n",
    "    row_dfnetfins = df_idx[df_idx['tag'].str.contains(\"dfnet\\\\+fins\")]\n",
    "    row_wpefins = df_idx[df_idx['tag'].str.contains(\"wpe\\\\+fins\")]\n",
    "\n",
    "    rows_fins_notanecho=pd.concat([row_dfnetfins,row_wpefins])\n",
    "\n",
    "    row_ourlogmelwave = df_idx[df_idx['tag'].str.contains(\"logmel\\\\+wave_0.8_0.2_chbest\")]\n",
    "    row_ourstftwave = df_idx[df_idx['tag'].str.contains(\"stft\\\\+wave_0.8_0.2_chbest\")]\n",
    "    row_ourlogmel = df_idx[df_idx['tag'].str.contains(\"logmel_1_chbest\")]\n",
    "    row_ourstft = df_idx[df_idx['tag'].str.contains(\"stft_1_chbest\")]\n",
    "\n",
    "    rows_ourbests=pd.concat([row_ourlogmelwave,row_ourstftwave, row_ourlogmel,row_ourstft])\n",
    "\n",
    "    row_ourstft0 = df_idx[df_idx['tag'].str.contains(\"ch0\")]\n",
    "    row_ourstft10 = df_idx[df_idx['tag'].str.contains(\"ch10\")]\n",
    "    row_ourstft50 = df_idx[df_idx['tag'].str.contains(\"ch50\")]\n",
    "\n",
    "    rows_ourhalftrained=pd.concat([row_ourstft0,row_ourstft10])\n",
    "    rows_ourmodels = df_idx[df_idx['tag'].str.contains(\"_ch\")]\n",
    "    rows_bestpreds = pd.concat([rows_ourbests,row_anechofins])\n",
    "\n",
    "    \n",
    "    # sTargetClone is more similar to sTarget than sContent (change in reverb)\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (row_content[col].item() >= row_targetclone[col].item()):  \n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone > sContent\")\n",
    "        elif \"S_\" in col:\n",
    "            if (row_content[col].item() <= row_targetclone[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone > sContent\")\n",
    "\n",
    "    # sTargetClone is most similar to sTarget from all the signals\n",
    "    for col in df_idx.columns:\n",
    "        sorted_values = df_idx.sort_values(by=col).reset_index(drop=True)\n",
    "        # loss/difference metrics\n",
    "        if \"L_\" in col  or \"D_\" in col: \n",
    "            if sorted_values.loc[sorted_values.index[0], 'tag']=='oracle -> tar:tarclone':  # Check second lowest\n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone most similar\")\n",
    "        # similarity metrics\n",
    "        elif \"S_\" in col: \n",
    "            if sorted_values.loc[sorted_values.index[-1], 'tag']=='oracle -> tar:tarclone':  # Check second lowest\n",
    "                note_metric_in_conddict(conditions_dict, \"sTargetClone most similar\")\n",
    "\n",
    "\n",
    "    # sContent less similar to target than sPred_anecho_fins and our well-trained models\n",
    "    # i.e. transformation helps\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_bestpreds[col] < row_content[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sContent < sPred_anecho_fins & our best models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_bestpreds[col] > row_content[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sContent < sPred_anecho_fins & our best models\")\n",
    "\n",
    "\n",
    "    # sPred_anecho_fins is better than all of our models\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_ourmodels[col] >= row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > all our models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_ourmodels[col] <= row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > all our models\")\n",
    "\n",
    "    # sPred_anecho_fins is at least better than logmel model \n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (row_ourlogmelwave[col] >= row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our logmel model\")\n",
    "        elif \"S_\" in col:\n",
    "            if (row_ourlogmelwave[col] <= row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our logmel model\")\n",
    "\n",
    "    # sPred_anecho_fins is at least better than our half-trained models \n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_ourhalftrained[col] >= row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our half-trained models\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_ourhalftrained[col] <= row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > our half-trained models\")\n",
    "\n",
    "    # sPred_anecho_fins is better than sPred_dfnet_fins and sPred_wpe_fins\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (rows_fins_notanecho[col] > row_anechofins[col].item()).all(): \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\")\n",
    "        elif \"S_\" in col:\n",
    "            if (rows_fins_notanecho[col] < row_anechofins[col].item()).all():  \n",
    "                note_metric_in_conddict(conditions_dict, \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\")\n",
    "\n",
    "\n",
    "    # stft+wave_0.8_0.2_checkpointbest better than logmel+wave_0.8_0.2_checkpointbest\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (row_ourlogmel[col].item() > row_ourstft[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\")\n",
    "        elif \"S_\" in col:\n",
    "            if (row_ourlogmel[col].item() < row_ourstft[col].item()):  \n",
    "                note_metric_in_conddict(conditions_dict, \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\")\n",
    "\n",
    "    # checkpoint0 <  checkpoint10 < checkpoint50\n",
    "    for col in df_idx.columns: \n",
    "        if \"L_\" in col  or \"D_\" in col:\n",
    "            if (row_ourstft0[col].item() > row_ourstft10[col].item() and \n",
    "            row_ourstft10[col].item() > row_ourstft50[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"earlier checkpoints < later checkpoints\")\n",
    "        elif \"S_\" in col:\n",
    "            if (row_ourstft0[col].item() < row_ourstft10[col].item() and \n",
    "            row_ourstft10[col].item() < row_ourstft50[col].item()): \n",
    "                note_metric_in_conddict(conditions_dict, \"earlier checkpoints < later checkpoints\")\n",
    "\n",
    "    return conditions_dict\n",
    "    # end of function \"check_requirements(df,idx)\"\n",
    "\n",
    "\n",
    "# get unique indices\n",
    "unique_idx=df.idx.unique()\n",
    "\n",
    "# initialize empty dictionary\n",
    "keys=[\"metric\",\n",
    "      \"idx\",\n",
    "      \"rev_delta\",\n",
    "      \"sTargetClone > sContent\",\n",
    "      \"sTargetClone most similar\",\n",
    "      \"sContent < sPred_anecho_fins & our best models\" ,\n",
    "       \"sPred_anecho_fins > all our models\" ,\n",
    "       \"sPred_anecho_fins > our logmel model\" ,\n",
    "       \"sPred_anecho_fins > our half-trained models\"  ,\n",
    "        \"sPred_anecho_fins > sPred_dfnet_fins & sPred_wpe_fins\" ,\n",
    "        \"stft+wave_0.8_0.2_checkpointbest > logmel+wave_0.8_0.2_checkpointbest\" ,\n",
    "       \"earlier checkpoints < later checkpoints\" ]\n",
    "combined_dict =  {key: [] for key in keys}\n",
    "\n",
    "i=0\n",
    "for idx in unique_idx:\n",
    "    i+=1\n",
    "    tmp_dict=check_requirements(df,idx)\n",
    "    combined_dict = {key: combined_dict[key] + tmp_dict[key] for key in keys}\n",
    "\n",
    "# go from dictionary to df\n",
    "df_table=pd.DataFrame(combined_dict)\n",
    "display(df_table.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  CREATE A TABLE INDICATING WHICH PERCEPTUAL OBSERVATIONS ARE CONFIRMED BY EACH METRIC ################### \n",
    "# CASE 1 -> ALLDATA\n",
    "\n",
    "needed_columns = df_table.drop(columns=[\"idx\",\"rev_delta\"],axis=1)\n",
    "df_table_g=needed_columns.groupby(\"metric\").agg(lambda x: int((x.sum() / len(x))*100)).reset_index()\n",
    "df_table_g = df_table_g.style.background_gradient(cmap='viridis', vmin=0, vmax=100,)\n",
    "display(df_table_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  CREATE A TABLE INDICATING WHICH PERCEPTUAL OBSERVATIONS ARE CONFIRMED BY EACH METRIC ################### \n",
    "# CASE 2 -> REV2DRY\n",
    "\n",
    "df_table_rev2dry=df_table[df_table[\"rev_delta\"]==\"rev2dry\"]\n",
    "\n",
    "needed_columns = df_table_rev2dry.drop(columns=[\"idx\",\"rev_delta\"],axis=1)\n",
    "df_table_g=needed_columns.groupby(\"metric\").agg(lambda x: int((x.sum() / len(x))*100)).reset_index()\n",
    "df_table_g = df_table_g.style.background_gradient(cmap='viridis', vmin=0, vmax=100,)\n",
    "display(df_table_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  CREATE A TABLE INDICATING WHICH PERCEPTUAL OBSERVATIONS ARE CONFIRMED BY EACH METRIC ################### \n",
    "# CASE 2 -> DRY2REV\n",
    "\n",
    "df_table_dry2rev=df_table[df_table[\"rev_delta\"]==\"dry2rev\"]\n",
    "\n",
    "needed_columns = df_table_dry2rev.drop(columns=[\"idx\",\"rev_delta\"],axis=1)\n",
    "df_table_g=needed_columns.groupby(\"metric\").agg(lambda x: int((x.sum() / len(x))*100)).reset_index()\n",
    "df_table_g = df_table_g.style.background_gradient(cmap='viridis', vmin=0, vmax=100,)\n",
    "display(df_table_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  CREATE A TABLE INDICATING WHICH PERCEPTUAL OBSERVATIONS ARE CONFIRMED BY EACH METRIC ################### \n",
    "# CASE 2 -> SMALLDIFF\n",
    "\n",
    "df_table_smalldiff=df_table[df_table[\"rev_delta\"]==\"smalldiff\"]\n",
    "\n",
    "needed_columns = df_table_smalldiff.drop(columns=[\"idx\",\"rev_delta\"],axis=1)\n",
    "df_table_g=needed_columns.groupby(\"metric\").agg(lambda x: int((x.sum() / len(x))*100)).reset_index()\n",
    "df_table_g = df_table_g.style.background_gradient(cmap='viridis', vmin=0, vmax=100,)\n",
    "display(df_table_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions: \n",
    "\n",
    "- Results for multiple audio samples are similar to the results for 1 audio sample\n",
    "- Most of the metrics \"see\" the difference in reverb (even when the difference in rt60 is small)\n",
    "- Metrics get confused when dealing with artificially processed sounds (for example, if the model was trained with the stft loss, the stft_loss(processed, target) is smaller than stft_loss(targetclone, target). This does not reflect the perception.\n",
    "- When delta in the rt60 is small, there is not much benefit in processing the data (see column \"sContent < sPred_anecho_fins & our best models\"), but when the rt60 delta is big it gets better. \n",
    "- When delta in the rt60 is small, there less benefit in training longer (see difference between early and late checkpoints)\n",
    "- It looks like for a large rt60 delta, the perceptual effect that the baseline sPred_anecho_fins is better than our models is sometimes confirmed by some metrics (see 3 middle columns). This perceptual effect is confirmed more often than when rt60 delta is small..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT AVERAGE METRICS \n",
    "\n",
    "# create df with average metric across data points\n",
    "numeric_df = df.select_dtypes(include='number')\n",
    "df_g=df.groupby(\"tag\")[numeric_df.columns].mean().reset_index()\n",
    "\n",
    "# plot average metrics\n",
    "plt.figure(figsize=(20,60))\n",
    "N_metrics=len(df_g.columns)\n",
    "for i, column in enumerate(df_g.columns):\n",
    "    if column not in [\"label\", \"idx\", \"compared\",\"short_label\",\"tag\"]:    \n",
    "        N_rows=int(np.ceil(N_metrics/3))\n",
    "        metriccolor=\"red\" if \"D_\" in column or \"L_\" in column else \"green\"\n",
    "        plt.subplot(N_rows,3,i+1)\n",
    "        bars=plt.bar(df_g[\"tag\"],df_g[column], color=metriccolor)\n",
    "        plt.xticks(rotation=60, ha='right', fontsize=\"9\")\n",
    "        plt.title(\"Metric mean: \" + column)\n",
    "        for j,bar in enumerate(bars):\n",
    "            val4clone=bars[j].get_height()#df_idx[column][df_idx[\"label\"]==\"sTarget : sAnecho\"][0]\n",
    "            plt.text(j,val4clone,str(\"%.2f\" % val4clone), horizontalalignment='center',verticalalignment='bottom', fontsize=\"9\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wave-u-net2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
