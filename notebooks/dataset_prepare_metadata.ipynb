{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT LIBRARIES ##################\n",
    "\n",
    "import numpy as np\n",
    "import random \n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT MY MODULES ##################\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import helpers as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# datapath=\"/media/ssd2/RESULTS-reverb-match-cond-u-net/\"\n",
    "datapath=\"/home/ubuntu/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(speech_pool)=93046\n"
     ]
    }
   ],
   "source": [
    "# ---------- SPEECH POOL ------------\n",
    "\n",
    "# create df with paths to speech files\n",
    "speech_dataset_path1 =  pjoin(datapath,'VCTK','wav48_silence_trimmed')\n",
    "speech_dataset_path2 =  pjoin(datapath,'PTDB')\n",
    "\n",
    "# initialize empty list of files:\n",
    "speech_pool = []\n",
    "\n",
    "# fill the list of files with filenames from vctk data base:\n",
    "database=\"VCTK\"\n",
    "for root, dirs, files in os.walk(speech_dataset_path1):\n",
    "    for file in files:\n",
    "        if file.endswith('.flac'):\n",
    "            # decide which split based on a probability \n",
    "            speech_pool.append({'database_speech': database, 'speech_file_path': os.path.join(root, file)})\n",
    "\n",
    "# fill the list of files with filenames from ptdb data base:\n",
    "database=\"PTDB\"\n",
    "for root, dirs, files in os.walk(speech_dataset_path2):\n",
    "    for file in files:\n",
    "        # make sure the correct speech files are used (MIC directory)\n",
    "        if (\"/MIC/\" in root) & (file.endswith('.wav')):\n",
    "            # decide which split based on a probability \n",
    "            speech_pool.append({'database_speech': database, 'speech_file_path': os.path.join(root, file)})  \n",
    "\n",
    "# shuffle order\n",
    "random.shuffle(speech_pool)\n",
    "\n",
    "# turn list to data frame \n",
    "speech_pool = pd.DataFrame(speech_pool)\n",
    "print(f\"{len(speech_pool)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rir_pool)=10000\n"
     ]
    }
   ],
   "source": [
    "# ---------- RIR POOL ------------\n",
    "rir_path=pjoin(datapath,'synth_rirs_mono')\n",
    "\n",
    "database=\"synth_rirs_mono\"\n",
    "# load df with rirs paths and stats (it was generated together with the RIRs - rir_dataset.ipynb):\n",
    "rir_pool=pd.read_csv(pjoin(rir_path,\"rir_info.csv\"),index_col=0)\n",
    "rir_pool[\"database_rir\"]=database\n",
    "print(f\"{len(rir_pool)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- CREATE METADATA FOR A DATASET BEING COMBINATIONS OF SPEECH AND RIRS --------\n",
    "from datetime import datetime\n",
    "date_tag = datetime.now().strftime(\"%d-%m-%Y--%H-%M\")\n",
    "# create dataset with 150000 data points, which consists of random combinations of speech, noise and rirs\n",
    "N_datapoints=150000\n",
    "\n",
    "# # sample from noise pool: \n",
    "# df_noise=noise_pool.sample(N_datapoints,replace=True)\n",
    "# # here plan ways to augment noise data set:\n",
    "# random_bool_values = [random.choice([1, -1]) for _ in range(len(df_noise))]\n",
    "# df_noise[\"aug_phase\"]=random_bool_values\n",
    "\n",
    "# sample from speech pool: \n",
    "df_speech=speech_pool.sample(N_datapoints,replace=True)\n",
    "# here plan ways to augment noise data set:\n",
    "random_bool_values = [random.choice([1, -1]) for _ in range(len(df_speech))]\n",
    "df_speech[\"aug_phase\"]=random_bool_values\n",
    "\n",
    "# sample from rir pool: \n",
    "df_rir=rir_pool.sample(N_datapoints,replace=True)\n",
    "\n",
    "# concatenate samples from speech, noise and rir pools\n",
    "# df_ds = pd.concat([df_speech.reset_index(drop=True), df_noise.reset_index(drop=True), df_rir.reset_index(drop=True)], axis=1,ignore_index=False)\n",
    "df_ds = pd.concat([df_speech.reset_index(drop=True), df_rir.reset_index(drop=True)], axis=1,ignore_index=False)\n",
    "\n",
    "df_ds = df_ds.reset_index(drop=True)\n",
    "\n",
    "# randomize snr (only high snrs)\n",
    "# df_ds[\"snr\"]= 200 #np.random.uniform(low=10, high=30, size=len(df_ds))\n",
    "\n",
    "# Create test-train-val split:\n",
    "df_ds.loc[0:N_datapoints*0.8,\"split\"]=\"train\" # 80% training data\n",
    "df_ds.loc[N_datapoints*0.8:N_datapoints*0.9,\"split\"]=\"test\" # 10% testing data\n",
    "df_ds.loc[N_datapoints*0.9:N_datapoints,\"split\"]=\"val\" # 10% validation data\n",
    "\n",
    "# save dataset metadata:\n",
    "# df_ds.to_csv(date_tag+\"_data_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change paths in the metadata from GuestXR computer: \n",
    "\n",
    "# df = pd.read_csv(\"../dataset-metadata/nonoise2_guestxr2.csv\",index_col=0)\n",
    "\n",
    "# df[\"speech_file_path\"]=df[\"speech_file_path\"].str.replace(\"/home/ubuntu/Data/\",\"/media/ssd2/\")\n",
    "# df[\"noise_file_path\"]=df[\"noise_file_path\"].str.replace(\"/home/ubuntu/Data/\",\"/media/ssd2/\")\n",
    "# df[\"ir_file_path\"]=df[\"ir_file_path\"].str.replace(\"/home/ubuntu/Data/\",\"/media/ssd2/\")\n",
    "\n",
    "# df.to_csv(\"../dataset-metadata/nonoise2_dacom.csv\",index=False)\n",
    "\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check generate database\n",
    "\n",
    "df = pd.read_csv(\"../dataset-metadata/17-05-2024--15-42_data_set.csv\",index_col=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
