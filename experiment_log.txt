
[2024-03-14-17:09:22]  testing new experiment log, pilot for symmetric conditioning and erly/late loss
[2024-03-14-17:28:05] testing new experiment log, pilot for symmetric conditioning and erly/late loss
[2024-03-15-14:49:39] debug
[2024-03-15-15:03:32] debug
[2024-03-15-15:14:44] debug early/late loss
[2024-03-15-15:32:23] same 
[2024-03-15-16:45:15] same 
[2024-03-18-10:18:39] corrected scaling of early/late
[2024-03-18-10:22:23] same
[2024-03-18-10:36:40] same
[2024-03-18-16:39:39] test ins2
[2024-03-18-16:42:43] test ins2
[2024-03-18-16:44:58] text ins2
[2024-03-18-16:47:19] test ins2
[2024-03-18-17:56:30] pilot for symmetric film and early/late reverb
[2024-03-18-17:57:14] same
[2024-03-18-17:57:41] same
[2024-03-18-17:58:37] same
[2024-03-18-18:02:53] same
[2024-03-18-18:04:54] same
[2024-03-18-18:06:34] logaudio to tensorflow has hardcoded indices of examples to use
[2024-03-18-18:15:55] same 
[2024-03-18-18:19:13] same, updated pilot dataset so it has validation split
[2024-03-18-18:20:22] same
[2024-03-19-09:52:27] corrected: alphas should not be in the combinations
[2024-03-19-09:53:39] same
[2024-03-19-09:55:08] same
[2024-03-19-09:55:41] same
[2024-03-19-09:58:44] pilot restart
[2024-03-19-10:27:12] big pilot (30 epochs with the full data set) 
[2024-03-20-11:52:05] valloss vs VALSTFT loss - in tensorboard they are the same 
[2024-03-20-11:54:34] same
[2024-03-20-11:59:11] same
[2024-03-20-12:11:48] same
[2024-03-20-12:51:57] checking how arguments are stored
[2024-03-20-18:27:09] extended pilot (100 epochs) for symmetric film and early/late loss 
[2024-03-25-12:43:48] stft+early+late did not work out well, trying another combination of alphas. emb loss helped a bit, trying it with stft loss only 
[2024-03-27-13:39:35] early/late reflections did not help - testing only late reflections regularizer and also additional logmel loss which from roc curves appears to be the most sensitive to reverb
[2024-03-28-17:53:14] pilot long training with new val losses 
[2024-03-28-19:33:28] same
[2024-03-28-19:34:30] same
[2024-03-28-19:35:08] same
[2024-03-28-19:41:28] long training stft, logmel and stft + logmel
[2024-04-02-15:26:31] testing variational wunet
[2024-04-02-15:27:27] same
[2024-04-02-17:34:08] same
[2024-04-06-00:00:05] is non-symmetric field better than symmetric one?
[2024-04-06-00:03:55] is symmetric film better than non-symmetric one
[2024-04-06-00:05:09] same
[2024-04-06-00:41:14] same
[2024-04-07-18:30:03] validation losses did not get logged so starting the same again
[2024-04-07-21:59:34] debugging tensorboard logger
[2024-04-07-22:02:37] tb is ok
[2024-04-08-13:01:44] short time
[2024-04-08-13:03:48] same
[2024-04-08-13:04:55] same
[2024-04-08-13:06:47] same
[2024-04-08-13:08:05] same
[2024-04-08-13:10:16] same
[2024-04-08-13:28:19] same
[2024-04-08-13:30:24] same 
[2024-04-08-16:31:13] variational wave-u-net
[2024-04-08-16:33:00] variational wave-u-net 
[2024-04-08-16:34:28] same
[2024-04-08-16:37:10] debugging variational 
[2024-04-08-16:42:51] run vae 
[2024-04-11-17:22:39] repeating bc of error in the losses (need to unpack output of vae)
[2024-04-12-22:39:29] repeat bc got disk too full
[2024-04-13-14:19:01] pilot for the stft training with lr adaptive and scheduled beta for vae loss
[2024-04-13-14:34:09] same 
[2024-04-13-14:35:05] sam
[2024-04-13-14:39:30] same
[2024-04-13-14:40:26] same
[2024-04-13-14:45:13] debug
[2024-04-13-21:19:46] same
[2024-04-13-21:24:40] debug
[2024-04-13-23:18:19] pilot for: new scheduling of learning rate, single optimizer, vae beta scheduling 
[2024-04-13-23:19:57] same
[2024-04-13-23:20:13] 
[2024-04-13-23:20:56]  
[2024-04-13-23:23:33] 
[2024-04-13-23:24:49] 
[2024-04-13-23:25:07] 
[2024-04-13-23:28:14] same
[2024-04-13-23:29:17] 
[2024-04-13-23:30:22] still debugging before pilot
[2024-04-13-23:31:46] 
[2024-04-13-23:32:25] 
[2024-04-13-23:35:19] 
[2024-04-13-23:46:54] same
[2024-04-14-00:21:01] big pilot (100 epochs) without beta scheduling 
[2024-04-14-00:22:46] same
[2024-04-17-15:26:36] pilot for perm_cond_losses=["stft+vae", "logmel+vae", "stft","logmel","stft+emb"]
[2024-04-17-15:28:17] same but 13 layers of encoder
[2024-04-17-15:29:13] same but layers of encoder=12
[2024-04-19-16:34:32] pilot (with pilot dataset) for different models 
[2024-04-19-16:35:33] same
[2024-04-19-17:15:00] for some reason there was the "late" loss in the experiment from 14-04... running again the same things (joint optimizer and adaptive learning rate) for stft and stft+vae and 100 epochs
[2024-04-22-12:31:00] testing with revenc 12, enc_len 128 and joint opt but without lr update
[2024-04-22-17:53:01] trying to recover the settings that lead to better results
[2024-04-23-17:10:51] bla
[2024-04-24-13:07:35] bla
[2024-04-24-13:10:34] bla
[2024-04-24-15:45:35] running wunet and varunet with noise, joint optim, after changing the config style
[2024-04-24-15:49:20] blabla
[2024-04-24-15:50:57] same
[2024-04-24-15:51:51] same
[2024-04-24-15:53:49] same
[2024-04-24-15:54:17] same
[2024-04-24-15:57:32] same 
[2024-04-24-16:02:20] same
[2024-04-24-16:06:34] same debug
[2024-04-24-16:12:54] same
[2024-04-24-16:14:25] same
[2024-04-24-16:16:03] pilot for the described thing
[2024-04-24-16:19:11] same
[2024-04-24-16:37:44] same
[2024-04-24-17:33:09] starting the real experiment - 4 conditions, 300 epochs each 
[2024-04-25-11:56:04] blA
[2024-04-25-11:57:58] BL
[2024-04-25-17:29:04] same but i realized this was doing a many-to-one transformation
[2024-04-25-18:16:07] same
[2024-05-08-12:17:09] testing fins
[2024-05-08-12:37:06] testing conditioned fins
[2024-05-08-17:00:12] testing again
[2024-05-08-17:01:11] same
[2024-05-08-23:20:45] pilot back to separated optimizer
[2024-05-08-23:30:08] back to old separation of optimizers, new: gauss len = 5 and enc-dec using gan tts
[2024-05-09-11:47:32] same
[2024-05-17-16:02:02] searching for the setting that works, increased datapoints 
[2024-05-17-16:02:56] pilot
[2024-05-17-16:04:00] pilot
[2024-05-17-16:09:01] pilot
[2024-05-17-16:10:11] pilot
[2024-05-17-16:14:42] pilot
[2024-05-17-16:25:58] logmel, logmel+wave and looking for the best setting...
[2024-05-17-16:28:27] same 
[2024-05-20-15:01:23] same but more epochs
[2024-06-10-09:59:55] the same as previous long experiment, appending this one into 20-05 - now testing with stft instead of logmel
[2024-06-10-10:04:49] debug
[2024-06-10-10:06:45] same
[2024-06-10-11:59:23] pilot 
[2024-06-10-12:41:49] pilot
[2024-06-10-13:00:59] pilot debug
[2024-06-10-13:06:27] bla
[2024-06-10-13:12:20] pilot debug
[2024-06-10-13:14:02] pilot
[2024-06-10-13:19:26] pilot
[2024-06-10-15:02:23] now full experiment 
[2025-02-14-17:11:10] pilot for the new dataset with varying src-rec dist and controlled rt60diff
[2025-02-14-17:12:47] same
[2025-02-14-17:13:30] same - fixed path in the basic.config
[2025-02-14-17:14:08] same
[2025-02-14-17:22:52] running pilot, 5 epochs each in tmux
[2025-02-17-11:28:22] full experiment with new data set
[2025-02-17-11:46:54] starting again, but only have time for 100 epochs 
[2025-03-11-11:05:39] 300 epochs, stft+wave and logmel+wave
[2025-03-16-11:23:51] long training(300 epochs) with wave losses
